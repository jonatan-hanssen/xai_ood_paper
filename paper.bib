@inproceedings{openood,
    title={Open{OOD}: Benchmarking Generalized Out-of-Distribution Detection},
    author={Jingkang Yang and Pengyun Wang and Dejian Zou and Zitang Zhou and Kunyuan Ding and WenXuan Peng and Haoqi Wang and Guangyao Chen and Bo Li and Yiyou Sun and Xuefeng Du and Kaiyang Zhou and Wayne Zhang and Dan Hendrycks and Yixuan Li and Ziwei Liu},
    booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2022},
    url={https://openreview.net/forum?id=gT6j4_tskUt}
}



@ARTICLE{tingsim,
  title    = "Machine learning in medicine: what clinicians should know",
  author   = "Ting Sim, Jordan Zheng and Fong, Qi Wei and Huang, Weimin and
              Tan, Cher Heng",
  abstract = "With the advent of artificial intelligence (AI), machines are
              increasingly being used to complete complicated tasks, yielding
              remarkable results. Machine learning (ML) is the most relevant
              subset of AI in medicine, which will soon become an integral part
              of our everyday practice. Therefore, physicians should acquaint
              themselves with ML and AI, and their role as an enabler rather
              than a competitor. Herein, we introduce basic concepts and terms
              used in AI and ML, and aim to demystify commonly used AI/ML
              algorithms such as learning methods including neural
              networks/deep learning, decision tree and application domain in
              computer vision and natural language processing through specific
              examples. We discuss how machines are already being used to
              augment the physician's decision-making process, and postulate
              the potential impact of ML on medical practice and medical
              research based on its current capabilities and known limitations.
              Moreover, we discuss the feasibility of full machine autonomy in
              medicine.",
  journal  = "Singapore Med J",
  volume   =  64,
  number   =  2,
  pages    = "91--97",
  month    =  may,
  year     =  2021,
  address  = "India",
  keywords = "Algorithms; artificial intelligence; deep learning; machine
              learning; neural networks",
  language = "en"
}

@article{dlmed,
author = {Alvin Rajkomar  and Jeffrey Dean  and Isaac Kohane },
title = {Machine Learning in Medicine},
journal = {New England Journal of Medicine},
volume = {380},
number = {14},
pages = {1347-1358},
year = {2019},
doi = {10.1056/NEJMra1814259},
URL = {https://www.nejm.org/doi/full/10.1056/NEJMra1814259},
eprint = {https://www.nejm.org/doi/pdf/10.1056/NEJMra1814259},
abstract = { In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. These data are collected and curated to provide the latest evidence-based assessment and recommendations. }
}


@misc{uncertainty,
      title={Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions}, 
      author={Eoin Delaney and Derek Greene and Mark T. Keane},
      year={2021},
      eprint={2107.09734},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.09734}, 
}

@InProceedings{generalxaiforood,
author="Sipple, John
and Youssef, Abdou",
editor="Ceci, Michelangelo
and Flesca, Sergio
and Masciari, Elio
and Manco, Giuseppe
and Ra{\'{s}}, Zbigniew W.",
title="A General-Purpose Method for Applying Explainable AI for Anomaly Detection",
booktitle="Foundations of Intelligent Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="162--174",
abstract="The need for explainable AI (XAI) is well established but relatively little has been published outside of the supervised learning paradigm. This paper focuses on a principled approach to applying explainability and interpretability to the task of unsupervised anomaly detection. We argue that explainability is principally an algorithmic task and interpretability is principally a cognitive task, and draw on insights from the cognitive sciences to propose a general-purpose method for practical diagnosis using explained anomalies. We define Attribution Error, and demonstrate, using real-world labeled datasets, that our method based on Integrated Gradients (IG) yields significantly lower attribution errors than alternative methods.",
isbn="978-3-031-16564-1"
}




@article{tallon2020explainable,
  title={Explainable AI: Using Shapley Value to Explain Complex Anomaly Detection ML-Based Systems},
  author={Tall{\'o}n-Ballesteros, AJ and Chen, C},
  journal={Machine Learning and Artificial Intelligence: Proceedings of MLIS 2020},
  volume={332},
  pages={152},
  year={2020},
  publisher={IOS Press}
}

@book{combood,
author = {Shekhar, Shashi and Papalexakis, Vagelis and Gao, Jing and Jiang, Zhe and Riondato, Matteo},
title = {Proceedings of the 2024 SIAM International Conference on Data Mining (SDM)},
publisher = {Society for Industrial and Applied Mathematics},
year = {2024},
doi = {10.1137/1.9781611978032},
address = {Philadelphia, PA},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611978032},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032}
}


@article{tcydenova2021detection,
  title={Detection of adversarial attacks in AI-based intrusion detection systems using explainable AI},
  author={Tcydenova, Erzhena and Kim, Tae Woo and Lee, Changhoon and Park, Jong Hyuk},
  journal={Human-Centric Comput Inform Sci},
  volume={11},
  year={2021}
}


@INPROCEEDINGS{martinez,
  author={Martinez-Seras, Aitor and Ser, Javier Del and Garcia-Bringas, Pablo},
  booktitle={2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Can Post-hoc Explanations Effectively Detect Out-of-Distribution Samples?}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  keywords={Heating systems;Analytical models;Training data;Machine learning;Predictive models;Data models;Stress measurement;Explainable Artificial Intelligence;Out-of-Distribution (OoD) detection;local explanations},
  doi={10.1109/FUZZ-IEEE55066.2022.9882726}}





@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@ARTICLE{dnsxai,
  author={Zebin, Tahmina and Rezvy, Shahadate and Luo, Yuan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={An Explainable AI-Based Intrusion Detection System for DNS Over HTTPS (DoH) Attacks}, 
  year={2022},
  volume={17},
  number={},
  pages={2339-2349},
  keywords={Tunneling;Servers;Security;Cryptography;Protocols;Computer crime;Feature extraction;Secure computing;machine learning;intrusion detection system;explainable AI},
  doi={10.1109/TIFS.2022.3183390}}


@article{mahbooba,
author = {Mahbooba, Basim and Timilsina, Mohan and Sahal, Radhya and Serrano, Martin},
title = {Explainable Artificial Intelligence (XAI) to Enhance Trust Management in Intrusion Detection Systems Using Decision Tree Model},
journal = {Complexity},
volume = {2021},
number = {1},
pages = {6634811},
doi = {https://doi.org/10.1155/2021/6634811},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/6634811},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/6634811},
abstract = {Despite the growing popularity of machine learning models in the cyber-security applications (e.g., an intrusion detection system (IDS)), most of these models are perceived as a black-box. The eXplainable Artificial Intelligence (XAI) has become increasingly important to interpret the machine learning models to enhance trust management by allowing human experts to understand the underlying data evidence and causal reasoning. According to IDS, the critical role of trust management is to understand the impact of the malicious data to detect any intrusion in the system. The previous studies focused more on the accuracy of the various classification algorithms for trust in IDS. They do not often provide insights into their behavior and reasoning provided by the sophisticated algorithm. Therefore, in this paper, we have addressed XAI concept to enhance trust management by exploring the decision tree model in the area of IDS. We use simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS. We experimented with this approach by extracting rules in a widely used KDD benchmark dataset. We also compared the accuracy of the decision tree approach with the other state-of-the-art algorithms.},
year = {2021}
}


@ARTICLE{idsxai,
  author={Arreche, Osvaldo and Guntur, Tanish R. and Roberts, Jack W. and Abdallah, Mustafa},
  journal={IEEE Access}, 
  title={E-XAI: Evaluating Black-Box Explainable AI Frameworks for Network Intrusion Detection}, 
  year={2024},
  volume={12},
  number={},
  pages={23954-23988},
  keywords={Explainable AI;Closed box;Network intrusion detection;Robustness;Malware;Network security;Detection algorithms;Internet security;Telecommunication traffic;Data science;Data models;Knowledge discovery;Data mining;NSL-KDD;XAI evaluation;intrusion detection systems;SHAP;explainable AI;network security;LIME;black-box AI;NSL-KDD;CICIDS-2017;RoEduNet-SIMARGL2021},
  doi={10.1109/ACCESS.2024.3365140}}

















@article{xaisurvey,
  title={Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks},
  author={Sajid Nazir and Diane M. Dickson and Muhammad Usman Akram},
  journal={Computers in biology and medicine},
  year={2023},
  volume={156},
  pages={
          106668
        },
  url={https://api.semanticscholar.org/CorpusID:257067347}
}

@misc{intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{performance,
  author    = {Dargan, Shaveta and Kumar, Munish and Ayyagari, Maruthi Rohit and Kumar, Gulshan},
  title     = {A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning},
  journal   = {Archives of Computational Methods in Engineering},
  volume    = {27},
  number    = {4},
  pages     = {1071--1092},
  year      = {2020},
  month     = {09},
  day       = {01},
  doi       = {10.1007/s11831-019-09344-w},
  issn      = {1886-1784},
  abstract  = {Nowadays, deep learning is a current and a stimulating field of machine learning. Deep learning is the most effective, supervised, time and cost efficient machine learning approach. Deep learning is not a restricted learning approach, but it abides various procedures and topographies which can be applied to an immense speculum of complicated problems. The technique learns the illustrative and differential features in a very stratified way. Deep learning methods have made a significant breakthrough with appreciable performance in a wide variety of applications with useful security tools. It is considered to be the best choice for discovering complex architecture in high-dimensional data by employing back propagation algorithm. As deep learning has made significant advancements and tremendous performance in numerous applications, the widely used domains of deep learning are business, science and government which further includes adaptive testing, biological image classification, computer vision, cancer detection, natural language processing, object detection, face recognition, handwriting recognition, speech recognition, stock market analysis, smart city and many more. This paper focuses on the concepts of deep learning, its basic and advanced architectures, techniques, motivational aspects, characteristics and the limitations. The paper also presents the major differences between the deep learning, classical machine learning and conventional learning approaches and the major challenges ahead. The main intention of this paper is to explore and present chronologically, a comprehensive survey of the major applications of deep learning covering variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects.},
  url       = {https://doi.org/10.1007/s11831-019-09344-w}
}







@misc{gradnorm,
      title={On the Importance of Gradients for Detecting Distributional Shifts in the Wild}, 
      author={Rui Huang and Andrew Geng and Yixuan Li},
      year={2021},
      eprint={2110.00218},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}











@misc{gbp,
      title={Striving for Simplicity: The All Convolutional Net}, 
      author={Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
      year={2015},
      eprint={1412.6806},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6806}, 
}

@misc{integratedgradients,
      title={Axiomatic Attribution for Deep Networks}, 
      author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
      year={2017},
      eprint={1703.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.01365}, 
}














@article{cifar,
  added-at = {2021-01-21T03:01:11.000+0100},
  author = {Krizhevsky, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5248afe57647d9c85c50a98a12145c/s364315},
  interhash = {cc2d42f2b7ef6a4e76e47d1a50c8cd86},
  intrahash = {fe5248afe57647d9c85c50a98a12145c},
  keywords = {},
  pages = {32--33},
  timestamp = {2021-01-21T03:01:11.000+0100},
  title = {Learning Multiple Layers of Features from Tiny Images},
  url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  year = 2009
}








# v1.5 report
@article{openood15,
  title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
  author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:2306.09301},
  year={2023}
}







@article{gradcam,
   title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
   volume={128},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-019-01228-7},
   DOI={10.1007/s11263-019-01228-7},
   number={2},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
   year={2019},
   month=oct, pages={336–359}
}

@misc{lime,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.04938}, 
}



@inproceedings{gradcamplusplus,
   title={Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks},
   url={http://dx.doi.org/10.1109/WACV.2018.00097},
   DOI={10.1109/wacv.2018.00097},
   booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
   publisher={IEEE},
   author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
   year={2018},
   month=mar }



@InProceedings{occlusion,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}
